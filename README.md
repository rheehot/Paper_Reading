# Paper_ReadingğŸ‘¨â€ğŸ“

#### If it has a problem, I 'll delete!

---

### S^3 DNN
Supervised Streaming and Scheduling for GPU - Accelerated Real-Time DNN Workloads

**Abstract:**

Deep Neural Networks (DNNs) are being widely applied in many advanced embedded systems that require autonomous decision making, e.g., autonomous driving and robotics. To handle resource-demanding DNN workloads, graphic processing units (GPUs) have been used as the main acceleration engine. Although much research has been conducted to algorithmically optimize the efficiency of applying DNN to applications such as object recognition, limited attention has been given to optimizing the execution of GPU-accelerated DNN workloads at the system level. In this paper, we propose S^3DNN, a system solution that optimizes the execution of DNN workloads on GPU in a real-time multi-tasking environment, which simultaneously optimizes the two (sometimes) conflicting goals of real-time correctness and throughput. S^3DNN contains a governor that selectively gathers system-wide DNN requests to perform smart data fusion, and a novel supervised streaming and scheduling framework that combines a deadline-aware scheduler with the concurrency-enabled CUDA stream technique. To simultaneously maximize concurrency-induced benefits and real-time performance, S^3DNN explores a rather interesting and unique characteristic of DNN workloads, where multiple layers of a DNN instance often exhibit a gradually decreased GPU resource utilization pattern. We have fully implemented S^3DNN in a GPU-accelerated system and have conducted extensive sets of experiments evaluating the efficacy of S^3DNN under a wide range of system and workload scenarios. The results show that S^3DNN significantly improves upon state-of-the-art GPU-accelerated DNN processing frameworks, e.g., up to 37% and over 40% improvements in real-time performance and throughput, respectively.

---

### YOLOv3
An Incremental Improvement 

**Abstract:**

We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network thatâ€™s pretty swell. Itâ€™s a little bigger than last time but more accurate. Itâ€™s still fast though, donâ€™t worry. At 320 x 320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 AP50 in 51 ms on a Titan X, com- pared to 57.5 AP50 in 198 ms by RetinaNet, similar perfor- mance but 3.8â‡¥ faster. As always, all the code is online at https://pjreddie.com/yolo/.

* 1 Stage Detector
* ë‹¨ í•˜ë‚˜ì˜ ë„¤íŠ¸ì›Œí¬ê°€ í•œ ë²ˆì— ê°ì²´ì˜ ìœ„ì¹˜ì™€ ë¶„ë¥˜ê°€ ë™ì‹œì— ì´ë£¨ì–´ì§„ë‹¤. ì¤‘ëŒ€í•œ localization error (ì‘ì€ BBì™€ í° BBë¥¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸), ì•µì»¤ë°•ìŠ¤ì˜ ê°œë… ë„ì…ìœ¼ë¡œë¶€í„° ì‚¬ì „ì— ì¢‹ì€ ì•µì»¤ ë°•ìŠ¤ë¥¼ ë¯¸ë¦¬ ì§€ì •í•´ì¤˜ì•¼í•œë‹¤. 
* ë°”ìš´ë”© ë°•ìŠ¤ì˜ í˜•íƒœê°€ íŠ¸ë ˆì´ë‹ ë°ì´í„°ë¥¼ í†µí•´ì„œë§Œ í•™ìŠµë˜ë¯€ë¡œ, ìƒˆë¡œìš´ / ë…íŠ¹í•œ í˜•íƒœì˜ ë°”ìš´ë”© ë°•ìŠ¤ì¸ ê²½ìš° ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ì§€ ëª»í•œë‹¤. ê²¹ì³ì§„ ì‚¬ë¬¼ì˜ êµ¬ë¶„ì€ ì–´ë µë‹¤.
* 7x7 ê·¸ë¦¬ë“œë¡œ ì˜ìƒì„ ë‚˜ëˆˆë’¤, ê° ê·¸ë¦¬ë“œì—ì„œ ì¤‘ì‹¬ì„ ê·¸ë¦¬ë“œ ì•ˆìª½ìœ¼ë¡œ í•˜ë©´ì„œ í¬ê¸°ê°€ ì¼ì •í•˜ì§€ ì•Šì€ ê²½ê³„ë°•ìŠ¤ë¥¼ 2ê°œì”© ìƒì„±.
* ê° ë°•ìŠ¤ëŠ” í•˜ë‚˜ì˜ box confidence scoreë¥¼ ê°€ì§€ê³  ìˆë‹¤. 
* í•œ ê·¸ë¦¬ë“œ ì…€ì´ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ë§Œ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤. ì‘ì€ ê°ì²´ ì—¬ëŸ¬ê°œê°€ ë‹¤ë‹¥ë‹¤ë‹¥ ë¶™ìœ¼ë©´ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•  ìˆ˜ ì—†ë‹¤. 
* YOLOëŠ” ê°ì²´íƒì§€ë¥¼ íšŒê·€ ë¬¸ì œë¡œ ì ‘ê·¼í•˜ë©° ë³„ë„ì˜ ì§€ì—­ ì œì•ˆì„ ìœ„í•œ êµ¬ì¡° ì—†ì´ í•œë²ˆì— ì „ì²´ ì´ë¯¸ì§€ë¡œë¶€í„° ì–´ë–¤ ê°ì²´ë“¤ì´ ì–´ë””ì— ìœ„ì¹˜í•˜ê³  ìˆëŠ”ì§€ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤. 
* YOLOëŠ” ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•œ ëª¨ë“  ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ë™ì‹œì— ì˜ˆì¸¡í•œë‹¤.
* NMS - confidence scoreìˆœìœ¼ë¡œ ì˜ˆì¸¡ì„ ì •ë ¬. ì˜ˆì¸¡ ì¤‘ì—ì„œ ì‹ ë¢°ë„ê°€ ê°€ì¥ í° ê²ƒ í•˜ë‚˜ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ ì§€ìš´ë‹¤. ì„ê³„ê°’ ë¯¸ë§Œë„ ë¬´ì‹œ
* YOLO-9000 : COCO ë°ì´í„°ì…‹ê³¼ ImageNet ë°ì´í„°ì…‹ì„ Joint í•˜ì—¬ ê¸°ì¡´ì˜ Object Detectionì„ ë‹¤ë£¨ëŠ” ëª¨ë¸ë“¤ ë³´ë‹¤ ë§ì€ ë°ì´í„°ë¥¼ í™œìš©í•˜ê² ë‹¤!
* YOLO-v2ëŠ” ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ Fully Connected Layerë¥¼ Convolution Layerë¡œ ë°”ê¿ˆ
* YOLOv3 : ê¸°ì¡´ì˜ YOLOì˜ recall ë¬¸ì œë¥¼ í•´ê²°í•˜ê² ë‹¤. recall ì´ë€ ê¸°ì¡´ì˜ YOLOì—ì„œ recall ì´ë€ detection rateì™€ ì¼ì¹˜. ì‚¬ì „ì— ì •ì˜ëœ Anchor Boxë¥¼ ì‹œë¯€ë„ì´ë“œ í•¨ìˆ˜ë¥¼ í†µí•˜ì—¬ BB ë¥¼ ë‹¤ì‹œ ì°¾ì•„ë‚´ê³  ê°€ì¥ GTì™€ ê°€ê¹Œìš´ ê°’ì„ Confidence ë¥¼ 1ë¡œ ì¤€ë‹¤. 

![img](https://blogfiles.pstatic.net/MjAxNzA0MjhfMTgy/MDAxNDkzMzYyNDk1MTE5.sJhub9RA2DgRz3-ziXYL-UfX1VcnPcpdxqzYoWnyTk4g.RtfPbkK1GajeoYLlXBpxotv6KZE_an22ns7C1OHlq00g.PNG.sogangori/last0.PNG?type=w1)

> ê·¸ë¦¼) ë„¤íŠ¸ì›Œí¬ ì˜ˆì¸¡

ì™¼ìª½ ë¹¨ê°„ì ìœ¼ë¡œ í‘œì‹œí•œ ë¶€ë¶„ì€ 7x7 ê·¸ë¦¬ë“œì…€ì¤‘ì— í•˜ë‚˜ë¡œ ì´ë¯¸ì§€ì—ì„œ ê°œì˜ ì¤‘ì•™ ë¶€ë¶„ì— í•´ë‹¹í•œë‹¤.

ê·¸ë¦¬ê³  ë¹¨ê°„ìƒ‰ ë°•ìŠ¤ë³´ë‹¤ í° **ë…¸ë€ìƒ‰ ë°•ìŠ¤**ê°€ ë°”ë¡œ ë¹¨ê°„ìƒ‰ ê·¸ë¦¬ë“œì…€ì—ì„œ ì˜ˆì¸¡í•œ ê²½ê³„ ë°•ìŠ¤ì´ë‹¤.

7x7 ì€ ì˜ìƒì„ 7x7 ì˜ ê²©ìë¡œ ë‚˜ëˆˆê²ƒì´ë‹¤.

30ê°œì˜ ì±„ë„ì€ (ê²½ê³„ ë°•ìŠ¤ì˜ ì •ë³´ 4ê°œ , ê²½ê³„ ë°•ìŠ¤ì•ˆì— ì˜¤ë¸Œì íŠ¸ê°€ ìˆì„ í™•ë¥ (confidence)) x 2, ì–´ë–¤ í´ë˜ìŠ¤ì¼ í™•ë¥  20ê°œ ë¡œ êµ¬ì„±ëœë‹¤.

ê²½ê³„ ë°•ìŠ¤ ì •ë³´ x, y : ë…¸ë€ìƒ‰ ê²½ê³„ ë°•ìŠ¤ì˜ ì¤‘ì‹¬ì´ ë¹¨ê°„ ê²©ì ì…€ì˜ ì¤‘ì‹¬ì—ì„œ ì–´ë””ì— ìˆëŠ”ê°€.

ê²½ê³„ ë°•ìŠ¤ ì •ë³´ w,h : ë…¸ë€ìƒ‰ ê²½ê³„ ë°•ìŠ¤ì˜ ê°€ë¡œ ì„¸ë¡œ ê¸¸ì´ê°€ ì „ì²´ ì´ë¯¸ì§€ í¬ê¸°ì— ì–´ëŠ ì •ë„ í¬ê¸°ë¥¼ ê°–ëŠ”ê°€

ë§Œì•½ ê²½ê³„ë°•ìŠ¤ê°€ ìœ„ì˜ ê·¸ë¦¼ì²˜ëŸ¼ ë˜ì—ˆë‹¤ë©´ x,yëŠ” ëª¨ë‘ 0.5 ì •ë„ì´ê³  w,hëŠ” ê°ê°  2/7, 4/7 ì •ë„ê°€ ë  ê²ƒì´ë‹¤.

ë…¸ë€ìƒ‰ ê²½ê³„ ë°•ìŠ¤ëŠ” ë°˜ë“œì‹œ ê·¸ ì¤‘ì‹¬ì´ ë¹¨ê°„ ê·¸ë¦¬ë“œ ì…€ ì•ˆì— ìˆì–´ì•¼ í•˜ë©°, ê°€ë¡œì™€ ì„¸ë¡œê¸¸ì´ëŠ” ë¹¨ê°„ ê·¸ë¦¬ë“œ ì…€ë³´ë‹¤ ì‘ì„ ìˆ˜ë„ ìˆê³  ê·¸ë¦¼ì²˜ëŸ¼ í´ ìˆ˜ë„ ìˆë‹¤.

ë˜í•œ ì •ì‚¬ê°í˜•ì¼ í•„ìš”ë„ ì—†ë‹¤.

ë¹¨ê°„ ê·¸ë¦¬ë“œ ì…€ ë‚´ë¶€ ì–´ë”˜ê°€ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•˜ì—¬ ê·¼ì²˜ì— ìˆëŠ” ì–´ë–¤ ì˜¤ë¸Œì íŠ¸ë¥¼ ë‘˜ëŸ¬ì‹¸ëŠ” ì§ì‚¬ê°í˜•ì˜ ë…¸ë€ìƒ‰ ê²½ê³„ ë°•ìŠ¤ë¥¼ ê·¸ë¦¬ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤.

ë…¸ë€ìƒ‰ ê²½ê³„ ë°•ìŠ¤ê°€ ë°”ë¡œ ROI , ì˜¤ë¸Œì íŠ¸ í›„ë³´ì´ë‹¤.

ì´ê²ƒì„ 2ê°œ ë§Œë“ ë‹¤.

Reference : [PR-207: YOLOv3: An Incremental Improvement](https://www.youtube.com/watch?v=HMgcvgRrDcA&list=PLqAFpvtCnrySi60YxMXf45YAyY9X24hLO&index=2)



---

### Mask R-CNN

**Abstract:**

We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach
efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each in- stance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in* parallel *with the existing branch for bounding box recogni- tion. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks,* e.g*., al- lowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.

* Faster R-CNN ì—ì„œ ì¶œë°œí•˜ì—¬ Instance Segmentaionì„ ì ìš©í•˜ê³ ì í•˜ëŠ” ëª¨ë¸
* Instance Segmentaion = Object detectionê³¼ semantic segmentationì„ ë™ì‹œì— í•´ì•¼ í•¨.
* Faster R-CNNì—ì„œ detectí•œ ê°ê°ì˜ boxì— maskë¥¼ ì”Œì›Œì£¼ëŠ” ëª¨ë¸
* Fully Convolutional Networks ì‚¬ìš©, Faster R-CNN + FCN = Mask R-CNN, 
* ê¸°ì¡´ì˜ Faster R-CNNì„ Object detection ì—­í• ì„ í•˜ë„ë¡ í•˜ê³  ê°ê°ì˜ RoI (Region of Interest)ì— mask segmentationì„ í•´ì£¼ëŠ” ì‘ì€ FCN (Fully Convolutional Network)ë¥¼ ì¶”ê°€
* RoI Pooling, RoIAlign ë‘˜ë‹¤ ì„œë¡œ ë‹¤ë¥¸ í¬ê¸°ì˜ regionì„ ë™ì¼í•œ í¬ê¸°ë¡œ ë§ì¶”ê¸° ìœ„í•´ ì‚¬ìš©. feature mapì„ cropí•˜ê³  ê³ ì •ëœ í¬ê¸°ë¡œ ë³´ê°„í•´ resizeí•œë‹¤
* RoI Pooling ê³¼ì •ì—ì„œ RoIê°€ ì†Œìˆ˜ì  ì¢Œí‘œë¥¼ ê°–ê³  ìˆì„ ê²½ìš°, ê° ì¢Œí‘œë¥¼ ë°˜ì˜¬ë¦¼í•œ ë‹¤ìŒì— Pooling, input Imageì˜ ì›ë³¸ ìœ„ì¹˜ ì •ë³´ê°€ ì™œê³¡ë˜ê¸° ë•Œë¬¸ì— Classification taskì—ì„œëŠ” ë¬¸ì œê°€ ë°œìƒí•˜ì§€ ì•Šì§€ë§Œ pixelë‹¨ìœ„ë¡œ detectioní•˜ëŠ” segmentaionì—ì„œëŠ” ë¬¸ì œê°€ ë°œìƒ.
* RoIAlign ìŒë°©í–¥ ë³´ê°„ì„ ì‚¬ìš©
* mask prediction , class predictionì„ decoupleí•¨. maskëŠ” ì–´ë–¤ classì¸ì§€ ëª°ë¼ë„ ëœë‹¤(binary mask)
* Loss function ì€ ë¶„ë¥˜, ë°•ìŠ¤ íšŒê·€, ë°”ì´ë„ˆë¦¬ ë§ˆìŠ¤í‚¹ì´ ë³‘í–‰ìœ¼ë¡œ ì²˜ë¦¬ëœë‹¤.

<img width="839" alt="ìŠ¤í¬ë¦°ìƒ· 2020-02-04 ì˜¤í›„ 6 11 05" src="https://user-images.githubusercontent.com/46750574/73739743-2d57a900-478a-11ea-94ba-162199909aed.png">



Reference : [PR-057: Mask R-CNN](https://www.youtube.com/watch?v=RtSZALC9DlU&t=881s)

---

### CornerNet

**Abstract:**

We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addi- tion to our novel formulation, we introduce corner pool- ing, a new type of pooling layer that helps the network better localize corners. Experiments show that Corner- Net achieves a 42.2% AP on MS COCO, outperforming all existing one-stage detectors.

* ê¸°ì¡´ì˜ Anchor Boxì˜ ë¬¸ì œì  
  * ì‚¬ì „ì— ë§Œë“¤ì–´ë†”ì•¼ í•œë‹¤. training time, inference time
  * positive, negative boxë“¤ì˜ imbalanceì„ ì´ˆë˜, high Recall
* bounding boxë¥¼ ì™¼ìª½ ìœ„, ì˜¤ë¥¸ìª½ ì•„ë˜ì˜ í•œ ìŒì˜ keyPointë¡œ ê°ì§€í•˜ëŠ” object detectionì— ëŒ€í•œ ìƒˆë¡œìš´ ì ‘ê·¼ë°©ë²•ì„ ì œì•ˆ
* ì´ keyPointë¡œ detectioní•˜ê¸° ë•Œë¬¸ì— ì•µì»¤ ë°•ìŠ¤ë¥¼ ë§Œë“¤ í•„ìš”ê°€ ì—†ë‹¤.
* Single convolution networkë¥¼ ì‚¬ìš©í•´ì„œ ëª¨ì„œë¦¬ì— ëŒ€í•œ heatmap, í•œ ìŒì˜ ëª¨ì„œë¦¬ë¥¼ ê·¸ë£¹í™” í•´ì¤„ ì„ë² ë”©ì„ ì˜ˆì¸¡í•œë‹¤
* conv Networkê°€ Corner localizeí•˜ëŠ”ë° ë„ì›€ì´ ë˜ëŠ” corner poolingì´ ì¡´ì¬
* Corner pooling

<img width="810" alt="ìŠ¤í¬ë¦°ìƒ· 2020-02-04 ì˜¤í›„ 7 18 29" src="https://user-images.githubusercontent.com/46750574/73739721-1fa22380-478a-11ea-94b4-62293b7d2d79.png">

<img width="541" alt="ìŠ¤í¬ë¦°ìƒ· 2020-02-04 ì˜¤í›„ 7 11 59" src="https://user-images.githubusercontent.com/46750574/73739752-2df03f80-478a-11ea-9691-2f541b8d3192.png">

* ë‘ ê°œì˜ ì˜ˆì¸¡ ëª¨ë“ˆ(top-left, bottom-right)ì´ ìˆë‹¤. 
* ê° ëª¨ë“ˆì—ëŠ” ìœ„ì— 3ê°€ì§€ë¥¼ ì˜ˆì¸¡í•˜ê¸° ì „ì— featureë¥¼ poolingí•˜ëŠ” corner poolingì´ ìˆë‹¤. 
* ê·¸ë¦¬ê³  objectë¥¼ detection í•˜ê¸° ìœ„í•´ì„œ ì—¬ëŸ¬ê°€ì§€ scaleë¥¼ ì‚¬ìš©í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤. backboneì˜ ì¶œë ¥ì—ë§Œ ë‘ ëª¨ë“ˆì„ ì ìš©í•œë‹¤.

<img width="289" alt="ìŠ¤í¬ë¦°ìƒ· 2020-02-04 ì˜¤í›„ 7 12 11" src="https://user-images.githubusercontent.com/46750574/73739704-1749e880-478a-11ea-9d62-350517196d6f.png">

* ê°ê°ì˜ cornerì— ëŒ€í•´ì„œ embedding vectorë¥¼ ì˜ˆì¸¡í•´ì„œ top-leftì™€ bottom-rightê°€ ë™ì¼í•œ bounding boxì— ì†í•˜ëŠ” ê²½ìš° embedding vector ì‚¬ì´ì˜ ê±°ë¦¬ê°€ ì‘ì•„ì•¼í•œë‹¤. 
* ê·¸ë¦¬ê³  cornerì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”ë¥¼ í•  ìˆ˜ ìˆë‹¤. embeddingì˜ ì‹¤ì œê°’ì€ ì¤‘ìš”í•˜ì§€ ì•Šê³  embedding ì‚¬ì´ì˜ ê±°ë¦¬ë§Œ ì¤‘ìš”í•˜ë‹¤. 1ì°¨ì› embeddingì„ ì‚¬ìš©í•œë‹¤.
* pull loss : Networkë¥¼ í›ˆë ¨í•´ cornerë¥¼ ê·¸ë£¹í™”
* push loss : Networkë¥¼ í›ˆë ¨í•´ cornerë¥¼ ë¶„ë¦¬
* ekek : etk,ebketk,ebkì˜ í‰ê· 
* âˆ† : 1



Reference : [PR-146: CornerNet](https://www.youtube.com/watch?v=6OYmOtivQY8&t=1433s)

---

### Fully Convolutional Networks for Semantic Segmentation 

**Abstract:**

Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolu- tional networks by themselves, trained end-to-end, pixels- to-pixels, exceed the state-of-the-art in semantic segmen- tation. Our key insight is to build â€œfully convolutionalâ€ networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolu- tional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [22], the VGG net [34], and GoogLeNet [35]) into fully convolu- tional networks and transfer their learned representations by fine-tuning [5] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed seg- mentations. Our fully convolutional network achieves state- of-the-art segmentation of PASCAL VOC (20% relative im- provement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.

* Fully convolutional networkë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ì´ë¯¸ì§€ì— ëŒ€í•œ í¬ê¸° ì œí•œì´ ë°œìƒí•˜ì§€ ì•ŠëŠ”ë‹¤
* ë…¼ë¬¸ì—ì„œëŠ” classificationìª½ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•œ ë„¤íŠ¸ì›Œí¬ë“¤ì„ ì‚¬ìš©í•¨. e.g VGGNet GoogLeNet..
* ê¸°ì¡´ê³¼ ë‹¬ë¦¬ convolution layerë§Œ ì‚¬ìš©ë¨ FC layer ëŒ€ì‹  1x1 Convolution layer ì‚¬ìš©
* ê¸°ì¡´ Fully Connected LayerëŠ” flattení•˜ê¸° ë•Œë¬¸ì— ìœ„ì¹˜ ì •ë³´ê°€ ì†Œì‹¤ë¨.
* ëŒ€ì‹  skip architecture êµ¬ì¡°ë¥¼ ì‚¬ìš©! FCNì„ ê±°ì¹˜ê³  ë‚˜ì˜¨ featureëŠ” coarse í•œ ìœ„ì¹˜ì •ë³´ë§Œì„ ê°€ì§€ê³ ìˆë‹¤.

<img width="636" alt="ìŠ¤í¬ë¦°ìƒ· 2020-02-04 ì˜¤í›„ 6 23 29" src="https://user-images.githubusercontent.com/46750574/73739751-2df03f80-478a-11ea-8877-f1621f3c24d8.png">

* FCNì˜ Architecture ë‹¨ê³„ëŠ” 3ë‹¨ê³„ë¡œ ë‚˜ë‰œë‹¤.
  * Featureë¥¼ ì¶”ì¶œí•˜ëŠ” Convolution ë‹¨ê³„
  * ë½‘ì•„ë‚¸ featureì— ëŒ€í•´ pixelwise prediction ë‹¨ê³„
  * classificationì„ í•œë’¤ ê° ì›ë˜ì˜ í¬ê¸°ë¡œ ë§Œë“¤ê¸° ìœ„í•œ Upsampling ë‹¨ê³„
  * ì´ëŸ¬í•œ ë‹¨ê³„ë¥¼ ê±°ì¹œ í›„ ê° pixelì— classë”°ë¼ ìƒ‰ì¹ ì„ í•œë’¤ Segmentation ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤. 

* 1 x 1 Convolution (Convolutionalization)ì„ í•˜ë©´ì„œ reshapeì´ ë˜ë¯€ë¡œ Upsampling ê³¼ì •ì´ í•„ìš”í•˜ë‹¤. ì—¬ê¸°ì„œ Skip Layerë¥¼ ì‚¬ìš©í•´ ë‹¤ì‹œ ì—…ìƒ˜í”Œë§ ê³¼ì •ê³¼ ìœ„ì¹˜ì •ë³´ë“¤ì„ ê°€ì ¸ì˜¨ë‹¤.
* DownSampling ê³¼ì •ì—ì„  semanticí•œ contextfulí•œ ì •ë³´ë“¤ì„ ì¶”ì¶œ

<img width="659" alt="ìŠ¤í¬ë¦°ìƒ· 2020-02-04 ì˜¤í›„ 6 31 56" src="https://user-images.githubusercontent.com/46750574/73739720-1fa22380-478a-11ea-809c-ac947c362a2a.png">

Reference: [Fully Convolutional Networks for Semantic Segmentation - í—ˆë‹¤ìš´](https://www.youtube.com/watch?v=_52dopGu3Cw&t=1112s)

---

### A Fully Automated system Using A Convolutional Neural Network to predict Renal Allograft Rejection: extra-validation with Giga-pixel Immunostained slides

**Abstract:**

Pathologic diagnoses mainly depend on visual scoring by pathologists, a process that can be time- consuming, laborious, and susceptible to inter- and/or intra-observer variations. This study proposes
a novel method to enhance pathologic scoring of renal allograft rejection. A fully automated system using a convolutional neural network (CNN) was developed to identify regions of interest (RoIs) and
to detect C4d positive and negative peritubular capillaries (PTCs) in giga-pixel immunostained slides. the performance of faster R-CNN was evaluated using optimal parameters of the novel method to enlarge the size of labeled masks. Fifty and forty pixels of the enlarged size images showed the best performance in detecting C4d positive and negative PTCs, respectively. Additionally, the feasibility
of deep-learning-assisted labeling as independent dataset to enhance detection in this model was evaluated. Based on these two CNN methods, a fully automated system for renal allograft rejection was developed. This system was highly reliable, efficient, and effective, making it applicable to real clinical workflow.

Reference: [A Fully Automated system Using A Convolutional Neural Network to predict Renal Allograft Rejection](https://www.nature.com/articles/s41598-019-41479-5)

* ë³‘ë¦¬ ì¡°ì§ ìŠ¬ë¼ì´ë“œë¥¼ íŒë…í•´ ì‹ ì¥ì´ì‹ ìˆ˜ìˆ  í›„ í•­ì²´ë§¤ê°œë©´ì—­ê±°ë¶€ë°˜ì‘ ì—¬ë¶€ë¥¼ ì§„ë‹¨í•´ë‚´ëŠ” ì¸ê³µì§€ëŠ¥ì„ ê°œë°œí•´ ì ìš©í•œ ê²°ê³¼, ë³‘ë¦¬ê³¼ ì „ë¬¸ì˜ê°€ ì§ì ‘ íŒë…í•œ ê²°ê³¼ ëŒ€ë¹„ 90%ì˜ ì •í™•ë„ë¥¼ ë³´ì˜€ë‹¤.
* íŒë… ì‹œê°„ë„ í‰ê· ì ìœ¼ë¡œ ì•½ 13ë¶„ì— ë¶ˆê³¼í•´ ì¸ê³µì§€ëŠ¥ìœ¼ë¡œ ì‹ ì¥ ì¡°ì§ì„ ë¶„ì„í•œ í›„ ë³‘ë¦¬ê³¼ ì „ë¬¸ì˜ê°€ ì¶”ê°€ì ìœ¼ë¡œ íŒë…í•˜ë©´ í˜¹ì‹œ ëª¨ë¥¼ ì§„ë‹¨ ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥ì„±ê³¼ ì§„ë‹¨ì— ê±¸ë¦¬ëŠ” ì‹œê°„ì´ í¬ê²Œ ì¤„ì–´ë“¤ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ëœë‹¤.
* ìˆ˜ìˆ  í›„ í•­ì²´ë§¤ê°œë©´ì—­ê±°ë¶€ë°˜ì‘ì´ ì˜ì‹¬ë˜ë©´ í™˜ìì˜ ì‹ ì¥ ì¡°ì§ì„ ì±„ì·¨í•œ í›„ íŠ¹ì • ë©´ì—­ì—¼ìƒ‰ ê¸°ë²•ì„ ì ìš©í•´ **ì„¸ë‡¨ê´€ ì£¼ìœ„ ëª¨ì„¸í˜ˆê´€(peritubular capillary)ì˜ ê°œìˆ˜**ë¥¼ ì„¸ì•¼ í•œë‹¤. ì—¼ìƒ‰ëœ ëª¨ì„¸í˜ˆê´€ì´ ì¼ì • ê¸°ì¤€ë³´ë‹¤ ë§ìœ¼ë©´ ì‹ ì¥ì´ì‹ ê±°ë¶€ë°˜ì‘ì´ ì‹¤ì œë¡œ ìˆì„ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ë‹¤ê³  ì§„ë‹¨í•œë‹¤.
* ì§€ê¸ˆê¹Œì§€ëŠ” ë³‘ë¦¬ê³¼ ì „ë¬¸ì˜ê°€ ì§ì ‘ í˜„ë¯¸ê²½ìœ¼ë¡œ ìˆ˜ë°± ë°° í™•ëŒ€í•´ ìœ¡ì•ˆìœ¼ë¡œ ë¶„ì„í•˜ê³  ìˆë‹¤. í•˜ì§€ë§Œ ëª¨ì„¸í˜ˆê´€ì´ ë§¤ìš° ë§ë‹¤ë³´ë‹ˆ ì¼ì¼ì´ ë‹¤ ë³´ëŠ” ê²ƒ ìì²´ê°€ ì‹œê°„ì´ ë§¤ìš° ì˜¤ë˜ ê±¸ë¦´ ë¿ë§Œ ì•„ë‹ˆë¼ ì¼ë¶€ë¶„ë§Œ ë³´ë”ë¼ë„ ëˆˆì— í”¼ë¡œê°€ ìŒ“ì—¬ ì •í™•ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ë°–ì— ì—†ëŠ” í•œê³„ê°€ ìˆì—ˆë‹¤.
* ì—°êµ¬íŒ€ì€ ë¨¼ì € 2009ë…„ë¶€í„° 2016ë…„ê¹Œì§€ ì„œìš¸ì•„ì‚°ë³‘ì›ì—ì„œ ì‹ ì¥ì´ì‹ ìˆ˜ìˆ ì„ ë°›ì€ í™˜ìë“¤ì˜ ì‹ ì¥ ë³‘ë¦¬ ì¡°ì§ ìŠ¬ë¼ì´ë“œ 200ê°œë¥¼ ë©´ì—­ì—¼ìƒ‰í•œ í›„ ì¸ê³µì§€ëŠ¥ì— í•™ìŠµì‹œì¼°ë‹¤.
* ì¸ê³µì§€ëŠ¥ì—ëŠ” ì¸ê°„ì˜ ì‹ ê²½ë§ì„ ë³¸ ëœ¬ í•©ì„±ê³± ì‹ ê²½ë§(CNN) ê¸°ìˆ ì´ ì ìš©ëìœ¼ë©° ì—°êµ¬íŒ€ì€ ì¶”ê°€ì ìœ¼ë¡œ 180ê°œì˜ ì‹ ì¥ ë³‘ë¦¬ ì¡°ì§ ìŠ¬ë¼ì´ë“œë¥¼ ì´ìš©í•´ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í–ˆë‹¤.
* ê·¸ ê²°ê³¼ ì‹ ì¥ ë³‘ë¦¬ ì¡°ì§ ìŠ¬ë¼ì´ë“œì—ì„œ ì„¸ë‡¨ê´€ ì£¼ìœ„ ëª¨ì„¸í˜ˆê´€ì´ ìˆì–´ ê¼­ ë¶„ì„í•´ì•¼ í•˜ëŠ” ì˜ì—­ë“¤ì„ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ì•½ 12ë¶„ ë§Œì— í‰ê·  147ê°œì”© ì°¾ì•„ëƒˆë‹¤.
* ë˜í•œ ê²€ì¶œëœ ì˜ì—­ ì¤‘ì—ì„œ ë³‘ë¦¬ê³¼ ì „ë¬¸ì˜ê°€ íŒë…í•œ ì •ë‹µ ëŒ€ë¹„ ë¬´ë ¤ 90%ì˜ ì •í™•ë„ë¡œ ì„¸ë‡¨ê´€ ì£¼ìœ„ ëª¨ì„¸í˜ˆê´€ì„ ì•½ 1ë¶„ ë§Œì— ì°¾ì•„ë‚´ ì´ 13ë¶„ ì •ë„ ë§Œì— ì‹ ì¥ì´ì‹ ê±°ë¶€ë°˜ì‘ ì—¬ë¶€ë¥¼ íŒë…í•´ëƒˆë‹¤. 
* Feasible RoIê²€ì¶œì„ ì‚¬ëŒì´ í–ˆì—ˆëŠ”ë° CNNì´ feasible , non-feasibleì„ ê²€ì¶œí•œë‹¤.
* ë’¤ì—ì„œëŠ” í›„ë³´ Feasible RoIë“¤ì„ ì¶”ì¶œí•˜ê³  ê·¸ì¤‘ì—ì„œ í›„ë³´ PTCë¥¼ ê²€ì¶œí–ˆë‹¤.



---

### CT Image Conversion among Different Reconstruction Kernels without a Sinogram by Using a Convolutional Neural Network

**Abstract:**

Objective: The aim of our study was to develop and validate a convolutional neural network (CNN) architecture to convert CT images reconstructed with one kernel to images with different reconstruction kernels without using a sinogram.
Materials and Methods: This retrospective study was approved by the Institutional Review Board. Ten chest CT scans were performed and reconstructed with the B10f, B30f, B50f, and B70f kernels. The dataset was divided into six, two, and two examinations for training, validation, and testing, respectively. We constructed a CNN architecture consisting of six convolutional layers, each with a 3 x 3 kernel with 64 filter banks. Quantitative performance was evaluated using root mean square error (RMSE) values. To validate clinical use, image conversion was conducted on 30 additional chest CT scans reconstructed with the B30f and B50f kernels. The influence of image conversion on emphysema quantification was assessed with Blandâ€“Altman plots. Results: Our scheme rapidly generated conversion results at the rate of 0.065 s/slice. Substantial reduction in RMSE was observed in the converted images in comparison with the original images with different kernels (mean reduction, 65.7%; range, 29.5â€“82.2%). The mean emphysema indices for B30f, B50f, converted B30f, and converted B50f were 5.4 Â± 7.2%, 15.3 Â± 7.2%, 5.9 Â± 7.3%, and 16.8 Â± 7.5%, respectively. The 95% limits of agreement between B30f and other kernels (B50f and converted B30f) ranged from -14.1% to -2.6% (mean, -8.3%) and -2.3% to 0.7% (mean, -0.8%), respectively. Conclusion: CNN-based CT kernel conversion shows adequate performance with high accuracy and speed, indicating its potential clinical use.

---

### DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs
**Abstract:**

In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or
â€˜atrous convolutionâ€™, as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed â€œDeepLabâ€ system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.

* field of view (í•œ í”½ì…€ì´ ë³¼ ìˆ˜ ìˆëŠ” ì˜ì—­)
* ì—¬íƒœ ë‚´ê°€ ë´¤ì—ˆë˜ segmentaion ë…¼ë¬¸ì¤‘ì—ì„  ì œì¼ ì¢‹ì€ë“¯ 2016ë…„ì— ë‚˜ì˜´
* Classificationì´ë‚˜ detectionì€ ê¸°ë³¸ì ìœ¼ë¡œ ëŒ€ìƒì˜ ì¡´ì¬ ì—¬ë¶€ì— ì§‘ì¤‘í•˜ê¸°ì— object-centricí•˜ë©°, ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ê¸° ìœ„í•´ì„  ì—¬ëŸ¬ ë‹¨ê³„ì˜ conv+pooling(classificationì€ globalí•œ ì •ë³´ë§Œ í•„ìš”í•˜ê¸° ë•Œë¬¸)ì„ ê±°ì³ ë§ ê·¸ëŒ€ë¡œ ì˜ìƒ ì†ì— ì¡´ì¬í•˜ë©° ë³€í™”ì— ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ”(robustí•˜ê²Œ ì˜í–¥ì„ ëœ ë°›ëŠ”) ê°•ì¸í•œ featureë§Œì„ ë„ì§‘ì–´ë‚´ì•¼ í•¨
  - ë”°ë¼ì„œ detailsë³´ë‹¤ëŠ” global í•œ ê²ƒì— ì§‘ì¤‘ì„í•´ì•¼ í•¨
* ë°˜ë©´ semantic segmentationì€ í”½ì…€ ë‹¨ìœ„ì˜ ì¡°ë°€í•œ ì˜ˆì¸¡ì´ í•„ìš”í•œë°, classification ë§ì„ ê¸°ë°˜ìœ¼ë¡œ segmantationë§ì„ êµ¬ì„±í•˜ê²Œ ë˜ë©´ ê³„ì† feature mapì˜ í¬ê¸°ê°€ ì¤„ì–´ë“¤ê¸°ì— detailí•œ ì •ë³´ë¥¼ ì–»ëŠ”ë° ì–´ë ¤ì›€ì´ ìˆìŒ
* ê·¸ë˜ì„œ FCN ê°œë°œìëŠ” skip layerë¥¼ ì‚¬ìš©í•˜ì—¬ 1/8, 1/16, 1/32 ê²°ê³¼ë¥¼ ê²°í•©(concat)í•˜ì—¬ detailì´ ì¤„ì–´ë“œëŠ” ë¬¸ì œë¥¼ ë³´ê°•í•˜ì˜€ìœ¼ë©°, DeepLabê³¼ ì•ì„œ ë³¸ dilated convolution íŒ€(Fisher Yu)ì€ ë§ì˜ ë’· ë‹¨ì— ìˆëŠ” 2ê°œì˜ pooling layerë¥¼ ì—†ì• ê³  dilated conv(atrous conv)ë¥¼ ì‚¬ìš©í•˜ì—¬ receptive fieldë¥¼ í™•ì¥ì‹œí‚¤ëŠ” íš¨ê³¼ë¥¼ ì–»ì—ˆìœ¼ë©°, 1/8 í¬ê¸°ê¹Œì§€ë§Œ feature mapì„ ì¤„ì´ë„ë¡ í•˜ì—¬ detailí•œ ì •ë³´ë“¤ì„ ë³´ì¡´í•¨
* í•˜ì§€ë§Œ 1/8ê¹Œì§€ë§Œ ì‚¬ìš©í•˜ë”ë¼ë„ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œê°€ ë°œìƒ
  - Receptive fieldê°€ ì¶©ë¶„íˆ í¬ì§€ ì•Šì•„ ë‹¤ì–‘í•œ scaleì— ëŒ€ì‘ì´ ì–´ë µë‹¤
  - 1/8í¬ê¸°ì˜ ì •ë³´ë¥¼ bilinear interpolationì„ í†µí•´ ì› ì˜ìƒ í¬ê¸°ë¡œ í‚¤ìš°ë©´ 1/32 í¬ê¸°ë¥¼ í™•ì¥í•œê²ƒë³´ë‹¤ëŠ” detailsê°€ ì‚´ì•„ìˆì§€ë§Œ ì—¬ì „íˆ ì •êµí•¨ì´ ë–¨ì–´ì§„ë‹¤. ë³´ê°„ì˜ í•œê³„
* Atrous ConvëŠ” ë³´ë‹¤ ë„“ì€ scaleì„ ë³´ê¸°ìœ„í•´ ì¤‘ê°„ì— hole(0)ì„ ì±„ì›Œë„£ê³  convì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ì˜ë¯¸
* kernel í¬ê¸°ëŠ” ê¸°ì¡´ì˜ Convì™€ ë™ì¼í•˜ê¸° ë•Œë¬¸ì— ì—°ì‚°ëŸ‰ì€ ë™ì¼í•˜ì§€ë§Œ receptive fieldì˜ í¬ê¸°ê°€ ì»¤ì§€ëŠ” íš¨ê³¼ê°€ ìˆë‹¤.
* DCNNì—ì„œ max-poolingì„ 2ê°œ ì œê±°í•¨ìœ¼ë¡œ detailì„ ì‚´ë¦¼. ì´ ìë¦¬ì— atrous convë¥¼ ì‚¬ìš©í•´ ë” ë„“ì€ receptive fieldë¥¼ ë³´ê²Œí•¨
* ê·¸ ì´í›„ë¡œ ìŒë°©í–¥ ë³´ê°„ìœ¼ë¡œ ì› ì˜ìƒ í¬ê¸°ë¥¼ ë³µì›í•¨. ê·¼ë° ë³´ê°„ë§Œìœ¼ë¡œëŠ” ì •í™•í•œ ê°ì²´ì˜ í”½ì…€ ë‹¨ìœ„ê¹Œì§€ ìœ„ì¹˜ë¥¼ ì •êµíˆ segmentationí•˜ëŠ”ê²Œ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ CRFë¥¼ ì´ìš©í•˜ì—¬ í›„ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ë„ë¡ í•¨
* ì „ì²´ì ì¸ êµ¬ì¡° = DCNN + CRFì´ë‹¤
* multi-scaleì— ë” ê°•ì¸í•˜ë„ë¡ fc6 Layerì—ì„œì˜ Atrous convë¥¼ ìœ„í•œ rateë¥¼ 6,12,18,24ë¡œ ì ìš©í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ concat ì‹œí‚´
* CRFëŠ” í”½ì…€ì˜ ìœ„ì¹˜, RGBê°’ìœ¼ë¡œëŠ” ë¹„ìŠ·í•˜ë‚˜ Labelì´ ë‹¤ë¥´ë©´ íŒ¨ë„í‹°ë¥¼ ì£¼ì–´ ê°œì„ ì‹œí‚¨ë‹¤.
* Short-range CRFëŠ” local connectionë§Œì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— detailí•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ì—†ë‹¤.
* fully connected CRFë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ detailí•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.
* ê·¼ë° fully connected CRFë¡œ ì „ ì˜ì—­ì„ ë‹¤ë³´ê¸°ì—” ì—°ì‚°ëŸ‰ì´ ë„ˆë¬´ ë§ì•„ì„œ mean field approximationì„ ì‚¬ìš©. 
* mean field approximation - ìˆ˜ë§ì€ ë³€ìˆ˜ë“¤ë¡œ ì´ë£¨ì–´ì§„ ë³µì¡í•œ ê´€ê³„ë¥¼ ê°–ëŠ” ìƒí™©ì—ì„œ íŠ¹ì • ë³€ìˆ˜ì™€ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì˜ ê´€ê³„ì˜ í‰ê· ì„ ì·¨í•˜ë©´, í‰ê· ìœ¼ë¡œë¶€í„° ë³€í™”ë¥¼ í•´ì„í•˜ëŠ”ë° ìš©ì´.

<img width="619" alt="ìŠ¤í¬ë¦°ìƒ· 2020-02-04 ì˜¤í›„ 6 52 40" src="https://user-images.githubusercontent.com/46750574/73739722-203aba00-478a-11ea-9a5c-a38ba4059374.png">

<img width="572" alt="ìŠ¤í¬ë¦°ìƒ· 2020-02-04 ì˜¤í›„ 7 03 08" src="https://user-images.githubusercontent.com/46750574/73739745-2d57a900-478a-11ea-9f37-3b447e30d703.png">

---

### U-Net: Convolutional Networks for Biomedical Image Segmentation 

* contracting path -> expanding path = encoding -> decoding
* Biomedical image Segmentationì—ì„œ ì“°ì´ê¸° ìœ„í•´ì„œ ì‚¬ìš©
* ë‹¨ìˆœí•œ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œë¥¼ ë„˜ì–´ì„œ ì´ë¯¸ì§€ì˜ íŠ¹ì • ì˜ì—­ì„ labelë¡œ í‘œí˜„í•˜ëŠ” image Segementationì— ëª©ì ì´ ìˆë‹¤.
* Sliding windowë¥¼ ì“°ì§€ì•Šê³  patchë°©ì‹ì„ ì±„íƒ. ì´ë¯¸ì§€ ì „ì²´ë¥¼ ê²©ìë¡œ ì˜ë¼ì„œ í•œë²ˆì— ì¸ì‹
* ê¸°ì¡´ì€ Context ì™€ localizationì˜ tradeoffì˜€ì§€ë§Œ Patchë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ê²°
* Patch - ì´ë¯¸ì§€ ì¸ì‹ ë‹¨ìœ„
  <img width="657" alt="ìŠ¤í¬ë¦°ìƒ· 2020-02-04 ì˜¤í›„ 7 26 23" src="https://user-images.githubusercontent.com/46750574/73739746-2d57a900-478a-11ea-9145-720a3e80d71d.png">
* ë§¤ contracting ë§ˆë‹¤ ê° max pool í•˜ê¸° ì „ ë ˆì´ì–´ì˜ ê²°ê³¼ê°’ì„ ìš°ì¸¡ì˜ ê°™ì€ ëŒ€ì‘ë˜ëŠ” í¬ê¸°ì˜ output í•„í„°ì— concatì‹œí‚´. ì™¼ìª½ ì´ë¯¸ì§€ê°€ ë” í¬ë¯€ë¡œ resizeí•´ì¤Œ. Mirroring paddingì„ ì§„í–‰í• ë•Œ ì†ì‹¤ë˜ëŠ” pathë¥¼ ì‚´ë¦¬ê¸° ìœ„í•´ì„œ ë³´ìƒì²˜ë¦¬ í•´ì¤Œ
* contracting pathì—ì„œ paddingì´ ì—†ì—ˆê¸° ë•Œë¬¸ì— ì ì  ì´ë¯¸ì§€ ì™¸ê³½ ë¶€ë¶„ì´ ì—†ì–´ì§
* ì´ë¯¸ì§€ê°€ ë‹¨ìˆœ ì‘ì•„ì§„ê²Œ ì•„ë‹ˆë¼ ì™¸ê³½ ë¶€ë¶„ì´ ì˜ë ¤ë‚˜ê°. ê·¸ë˜ì„œ mirroringìœ¼ë¡œ ì´ë¯¸ì§€ ë³µêµ¬
* ë‚´ë ¤ê°ˆë•ŒëŠ” 1/2 down sampling , í™œì„±í™” í•¨ìˆ˜ëŠ” ReLU

<img width="557" alt="ìŠ¤í¬ë¦°ìƒ· 2020-02-04 ì˜¤í›„ 7 26 51" src="https://user-images.githubusercontent.com/46750574/73739753-2df03f80-478a-11ea-9ed5-0f6e7117257a.png">

* wë¼ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ì •ë‹µ í”½ì…€ì— ëŒ€í•œ cross entropy lossì— ì¶”ê°€
* d1ì€ ê°€ì¥ ê°€ê¹Œìš´ ì…€, d2ëŠ” ë‘ë²ˆì§¸ë¡œ ê°€ê¹Œìš´ ì…€
* ë‘ ì„¸í¬ì‚¬ì´ì˜ ê°„ê²©ì´ ì¢ì„ ìˆ˜ë¡ weightë¥¼ í° ê°’ìœ¼ë¡œ ë‘ ì„¸í¬ ì‚¬ì´ê°€ ë„“ì„ ìˆ˜ë¡ weightë¥¼ ì‘ì€ ê°’ìœ¼ë¡œ ê°–ê²Œ ëœë‹¤. 
* ê°„ê²©ì´ ë„“ì–´ì§ˆ ìˆ˜ë¡ loss ê°€ ì‘ì•„ì§„ë‹¤.
* ê° cellë“¤ì´ ë§Œë‚˜ëŠ” ê²½ê³„ë¶€ë¶„ì—ì„œì˜ ê°€ì¤‘ì¹˜ë¥¼ ë” ë†’ê²Œ ì£¼ì–´ì„œ ê²½ê³„ë¶€ë¶„ì„ í™•ì‹¤í•˜ê²Œ êµ¬ë¶„í•´ ë‚´ê² ë‹¤ëŠ” ê²ƒ.

![u-net_fig_2](https://modulabs-biomedical.github.io/assets/images/posts/2018-04-02-U_Net/u-net_fig_2.png)

> Overlap-tile ì „ëµì€, U-Netì—ì„œ ë‹¤ë£¨ëŠ” ì „ì í˜„ë¯¸ê²½ ë°ì´í„°ì˜ íŠ¹ì„±ìƒ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆì˜ í¬ê¸°ê°€ ìƒë‹¹íˆ í¬ê¸° ë•Œë¬¸ì— Patch ë‹¨ìœ„ë¡œ ì˜ë¼ì„œ Input ìœ¼ë¡œ ë„£ê³  ìˆë‹¤.
>
> ì´ë•Œ `Fig.2`ì—ì„œ ë³´ëŠ” ê²ƒê³¼ ê°™ì´ Border ë¶€ë¶„ì— ì •ë³´ê°€ ì—†ëŠ” ë¹ˆ ë¶€ë¶„ì„ 0ìœ¼ë¡œ ì±„ìš°ê±°ë‚˜, ì£¼ë³€ì˜ ê°’ë“¤ë¡œ ì±„ìš°ê±°ë‚˜ ì´ëŸ° ë°©ë²•ì´ ì•„ë‹Œ Mirroring ë°©ë²•ìœ¼ë¡œ pixelì˜ ê°’ì„ ì±„ì›Œì£¼ëŠ” ë°©ë²•.
>
> ë…¸ë‘ìƒ‰ ì˜ì—­ì´ ì‹¤ì œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë  ì˜ì—­ì´ê³ , íŒŒë‘ìƒ‰ ë¶€ë¶„ì´ Patch.
>
> ê·¸ë¦¼ì„ í™•ëŒ€í•´ì„œ ìì„¸íˆ ë³´ì‹œë©´, ê±°ìš¸ì²˜ëŸ¼ ë°˜ì‚¬ë˜ì–´ borderë¶€ë¶„ì´ ì±„ì›Œì§„ ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤.

---

### Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer

SLN ì´ë€? sentinel axillary lymph nodes ë¡œì¨ ìœ ë°©ì•” ì§„ë‹¨ì˜ ì¤‘ìš”í•œ ë¶€ë¶„ì´ë‹¤. SLN ì§„ë‹¨ì€ ë§¤ìš° ì§€ì¹˜ê³  ì‹œê°„ì´ ë§ì´ ì†Œë¹„ë¨. ë”°ë¼ì„œ ì´ ì—°êµ¬ì—ì„œëŠ” ë”¥ëŸ¬ë‹ì„ í™œìš©í•˜ì—¬ SLNìŠ¬ë¼ì´ë“œ ì•ˆì—ì„œì˜ ì „ì´ ê°ì§€ë¥¼ ì¡°ì‚¬í•˜ê³  ì‹¤ì œ ë³‘ë¦¬í•™ìì˜ ì§„ë‹¨ê³¼ ë¹„êµí•œë‹¤. Dataset ì€ ì¹´ë©œë¼ë‹ˆì•ˆ 16 ì‚¬ìš© 11ëª…ì˜ ë³‘ë¦¬í•™ìê°€ ì°¸ì—¬í•˜ì—¬ ë¹„êµí•´ì¤Œ.

íŠ¸ë ˆì´ë‹ì€ 270 í…ŒìŠ¤íŠ¸ëŠ” 129 . ë‘ í…ŒìŠ¤í¬ë¡œ ë‚˜ë‰˜ëŠ”ë° í•˜ë‚˜ëŠ” wsiì´ë¯¸ì§€ë¡œ ê°œë³„ ì „ì´ì‹ë³„ê³¼ ëª¨ë“  wsiì´ë¯¸ì§€ì—ì„œ SLN ì „ì²´ ìŠ¬ë¼ì´ë“œ ì˜ìƒì—ì„œ ê°œë³„ì ì¸ ì „ì´ í™•ì¸ê³¼ ëª¨ë“  wsiì˜ìƒì—ì„œì˜ SLNì˜ ì „ì´ë¥¼ í¬í•¨í•˜ê±°ë‚˜ í¬í•¨í•˜ì§€ ì•Šì€ê²ƒìœ¼ë¡œ ì •ì˜í•¨.

í…ŒìŠ¤í¬ 1 ì€ FROCë¡œ í‰ê°€ë¥¼ ë°›ê³  , í…ŒìŠ¤í¬ 2 ëŠ” 49ê°œì˜ SLNí¬í•¨ëœ WSIì™€ 80ê°œì˜ í¬í•¨ì•ˆëœ ì´ë¯¸ì§€ã…‡ë¥¼ êµ¬ë³„í•˜ë„ë¡ í‰ê°€ë°›ìŒ í‰ê°€ëŠ” ROCë¡œ í•¨. ë˜í•œ ì•Œê³ ë¦¬ì¦˜ì€ ì‹œê°„ì— ì œì•½ì„ ë‘ëŠ” ë³‘ë¦¬í•™ìì™€ ì•ˆë‘ëŠ” ë³‘ë¦¬í•™ìì™€ ë˜‘ê°™ì´ êµ¬ë¶„í•˜ì—¬ í‰ê°€ë°›ìŒ. ì‹œê°„ ì œì•½ì€ 2ì‹œê°„ìœ¼ë¡œ 129ê°œì˜ wsië¥¼ í‰ê°€ë°›ìŒ .

ì‹ ë¢°ë„ëŠ” 5ë‹¨ê³„(í™•ì‹¤íˆ ì •ìƒ, ì•„ë§ˆë„ ì •ìƒ, ëª¨í˜¸, ì•„ë§ˆë„ ì¢…ì–‘, í™•ì‹¤íˆ ì¢…ì–‘)ë¡œ ë‚˜íƒ€ëƒˆë‹¤.

#### level confidence 

* definitely normal
* probably normal
* equivocal
* probably tumor
* definitely tumor

í…ŒìŠ¤í¬ 1,2 ì˜ ì•Œê³ ë¦¬ì¦˜ì—ì„œ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ì ì„ ë³´ì¸ íŒ€ì€ í•˜ë²„ë“œ mit íŒ€ì¸ë° ì´ íŒ€ì€ í…ŒìŠ¤í¬ 1ì€ FROC 0.807ì´ì—ˆë‹¤. í…ŒìŠ¤í¬ 2ì—ì„œëŠ” AUC 0.994ë¡œ ì œì¼ ë†’ì•˜ë‹¤. 1ë“±íŒ€ì€ êµ¬ê¸€ë„·ì„ ì‚¬ìš©í–ˆë‹¤. WTC ë³‘ë¦¬í•™ìì˜ AUCëŠ” 0.810ì´ì—ˆë‹¤. ê·¸ë¦¬ê³  ìƒìœ„ 20ìœ„ê¹Œì§€ì˜ í…ŒìŠ¤í¬ ë¶„ë¥˜ ë°©ì‹ì´ ë‹¤ ë”¥ëŸ¬ë‹ì´ì—ˆëŠ”ë°ë„ ë¶ˆêµ¬í•˜ê³  ì„±ëŠ¥ì˜ ì°¨ì´ê°€ ë‚œê²ƒì€ ë³´ì¡° ì „ëµë•Œë¬¸ì´ì—ˆëŠ”ë° standardization technique ê³¼ ê°™ì€ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì˜€ë‹¤. 

#### task 1 Identification

#### task 2 Classification

#### Limitation

* ì •ìƒê³¼ ì¢…ì–‘ ì¡°ì§ì˜ ë°°ê²½ì‚¬ì´ì— êµ¬ë¶„í•˜ê¸°ìœ„í•œ í›ˆë ¨ì´ í•„ìš”ëœë‹¤
* SLNì—ì„œì˜ ë‹¤ë¥¸ ë³‘ë¦¬ ê°ì§€ëŠ” ì œì™¸ë¨
* ì•Œê³ ë¦¬ì¦˜ì˜ ëŸ°íƒ€ì„ì€ ê¸°ë¡ë˜ì§€ ì•ŠìŒ
* ì„ìƒì‹¤í—˜ì‹œ ë³‘ë¦¬í•™ìê°€ ì ‘í•˜ëŠ” ì‚¬ë¡€ì™€ ì§ì ‘ì ìœ¼ë¡œ ë¹„êµê°€ ë˜ì§€ ì•ŠëŠ”ë‹¤.
* ë³‘ë¦¬í•™ìì˜ ìŠ¬ë¼ì´ë“œì—ëŠ” ìœ ë°©ì•” ì „ì´ì˜ ì¡´ì¬, ë¶€ì¬ë¥¼ ê²°ì •í• ìˆ˜ìˆëŠ” í™˜ìë§ˆë‹¤ í•œ ê°œì˜ ì—¼ìƒ‰ ìŠ¬ë¼ì´ë“œë§Œ ì£¼ì–´ì§. 
* í˜„ì‹¤ ì˜ë£Œì—ì„œëŠ” ë‹¤ì¤‘ë ˆë²¨ì˜ ì„¹ì…˜ìœ¼ë¡œë¶€í„° í‰ê°€í•¨

---

## Detecting Cancer Metastases on Gigapixel Pathology Images

#### í¬ê²Œ slide-level Classification ê³¼ tumor-level classificationìœ¼ë¡œ ë‚˜ë‰¨.

* ìœ ë°©ì•” í™˜ìì— ëŒ€í•œ ì¹˜ë£Œì˜ ê²°ì •ì€ ìœ ë°©ìœ¼ë¡œë¶€í„° ë‹¤ë¥¸ ì¡°ì§ìœ¼ë¡œ ì „ì´ê°€ ë˜ì—ˆëŠ”ì§€ì˜ ì—¬ë¶€ì— ë‹¬ë ¤ìˆë‹¤. ë³‘ë¦¬í•™ìë“¤ì€ ì‹œê°„ê³¼ ë…¸ë ¥ì„ ìŸì•˜ì§€ë§Œ ì—¬ì „íˆ ì˜¤ì§„ê³¼ ê°•ë„ë†’ì€ ë…¸ë™ì´ í•„ìš”
* CNN ê³¼ Camelyon16 ì—ì„œì˜ sotaì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ 97% AUCì™€ 2ê°œì˜ ì˜ëª» ë¼ë²¨ë§ëœ ë°ì´í„°ë¥¼ ë°œê²¬, FNë„ ì¤„ì„
* ë¦¼í”„ì ˆì—ì„œ ìœ ë°©ì•” ì „ì´ ë°œê²¬ì„ ë•ëŠ” í”„ë ˆì„ì›Œí¬ (CNN)
* Inception êµ¬ì¡°ë¡œ stride 128ë¡œ ë°”ê¿§ëŠ”ë° ìŠ¬ë¼ì´ë“œë§ˆë‹¤ 8 FPë¡œ ì ˆë°˜ì´ë‚˜ ì¤„ì„
* ìš°ë¦¬ëŠ” ì´ìµì´ ì—†ëŠ” ì—¬ëŸ¬ ì ‘ê·¼ë²•ì„ ë°œê²¬í•¨
  * ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì ‘ê·¼
  * ì´ë¯¸ì§€ë„· ì´ë¯¸ì§€ì˜ pretraining
  * color normalization
    * ë””ì§€í„¸ ì˜ìƒì— ëŒ€í•œ ì˜ìƒì²˜ë¦¬ ê³¼ì •ì˜ ì •í™•ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ì›í•˜ì§€ ì•ŠëŠ” ë’¤í‹€ë¦¼ì„ ì¤„ì´ê³  ì¼ë¶€ íŠ¹ì§•ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ì²˜ë¦¬ë¥¼ ì˜ë¯¸í•œë‹¤[12]. ì¡°ì§ë³‘ë¦¬ ì˜ìƒì˜ ê²½ìš°, ì—¼ìƒ‰ê³¼ì •ê³¼ ìŠ¤ìºë‹ ì¡°ê±´ì— ë”°ë¥¸ í¸ì°¨ë¥¼ ì¤„ì´ê¸° ìœ„í•´ â€˜ìƒ‰ìƒ ì •ê·œí™”(color normalization)â€™ ì‘ì—…ì´ í•„ìˆ˜ì ì´ë‹¤(ê·¸ë¦¼1). ë˜í•œ ë©´ì—­í˜•ê´‘ì—¼ìƒ‰ì˜ ê²½ìš° ì¡°ì§ì˜ ìê°€í˜•ê´‘ì„ ë³´ìƒí•˜ê¸° ìœ„í•œ ì‘ì—…ë„ ì „ì²˜ë¦¬ ê³¼ì •ìœ¼ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤[13].
    * í•„ìëŠ” ì„±ê³¼ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ì •ê·œí™”ë¥¼ ë°œê²¬í•˜ì§€ ëª»í–ˆë‹¤.
* Inception v3ì„ ì‚¬ìš©, input size = 299 x 299. ê° ì¸í’‹ì˜ íŒ¨ì¹˜ì—ì„œ 128 x 128 ì¤‘ì‹¬ ì˜ì—­ì˜ ë¼ë²¨ì„ ì˜ˆì¸¡. ê·¸ ì¤‘ì—ì„œ í•˜ë‚˜ë¼ë„ ì¢…ì–‘ì´ ì¡´ì¬í•œë‹¤ë©´ ì¢…ì–‘ì´ë¼ê³  ë¼ë²¨ë§ í•¨
* paramìˆ˜ë¥¼ ì¤„ì—¬ê°€ë©° ì˜í–¥ ì‹¤í—˜ì„ í•´ë´„. ë˜í•œ multi scale ì€ ë³„ë¡œ íš¨ê³¼ê°€ ì—†ì—ˆê¸° ë•Œë¬¸ì— 2ê°œì˜ í¬ê¸°ë¡œë§Œ ì‚¬ìš©

Sampling - tumor ì™€ normal ì˜ ë¹„ìœ¨ì´ imbalance í•˜ê¸° ë•Œë¬¸ì— carefulí•˜ê²Œ ìƒ˜í”Œë§í•¨

* normal, tumor ë¥¼ ê°™ì€ í™•ë¥ ë¡œ ë½‘ìŒ
* labelì˜ patchë¥¼ ê°€ì§€ê³ ìˆëŠ” slideë¥¼ randomí•˜ê²Œ ë½‘ìŒ
* ê·¸ patchë“¤ì„ ê°€ì§€ê³  sampling

#### Data Augmentations

- 4 multiples of 90Ëš rotations + left-right flip (8 orientations)
- perturb color : (maximum delta)
  - brightness 64/255,
  - saturation 0.25,
  - hue 0.04
  - contrast 0.75
- Jitter : up to 8 pixels.
- pixel values clipped [0,1], and scaled [-1,1]

#### Implementations Details

- batch size = 32
- RMSProp , momentum 0.9, decay 0.9, $\epsilon$ = 1.0
- initial lr = 0.05, 0.5 decay every 2 M ex.
- for refining a model pretrained on ImageNet, initial lr = 0.002

FROC computation ì„ ìœ„í•´ ì¹´ë©œë ˆì˜¨ ìŠ¹ìë“¤ì€ ë¹„íŠ¸ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ì—¬ íˆíŠ¸ë§µì— ì„ê³„ê°’ì„ ì ìš©, ë¹„íŠ¸ë§µì˜ ê° ì—°ê²°ìš”ì†Œë“¤ì„ ë‹¨ì¼ ì˜ˆì¸¡ì„ reportí•¨.

ì´ì™€ëŠ” ë°˜ëŒ€ë¡œ, í•„ìëŠ” ë¹„ìµœëŒ€ì–µì œë¥¼ ì‚¬ìš©í•˜ì—¬ të¥¼ ë„˜ëŠ” íˆíŠ¸ë§µì•ˆì— ìˆëŠ” ê°’ì´ ì—†ì„ë•Œ ê¹Œì§€ ë‘ ë‹¨ê³„ë¥¼ ë°˜ë³µí•¨. 

ì´ì „ ì—°êµ¬ë“¤ì€ pre-training ëœ ë‹¤ë¥¸ ë„ë©”ì¸ì—ì„œì˜ ì„±ëŠ¥ì„ ë³´ì¸ ëª¨ë¸ë“¤ì„ ì‚¬ìš©í–ˆë‹¤. í•˜ì§€ë§Œ í•„ìëŠ” pre-trainingì´ convergence speed ì—ì„œ improve í•  ìˆœ ìˆìœ¼ë‚˜, FROCì„ ê°œì„ í•˜ì§„ ëª»í•¨. ë³‘ë¦¬í•™ ì´ë¯¸ì§€ì™€ ImageNetì—ì„œì˜ imageë“¤ì€ í° ë„ë©”ì¸ì˜ ì°¨ì´ê°€ ìˆê¸° ë•Œë¬¸ì´ë¼ê³  ì¶”ì¸¡. ê²Œë‹¤ê°€ ë³‘ë¦¬í•™ì˜ ë°ì´í„°(í•„ìì˜ ë°ì´í„°ëŠ” 10^7 patches) ì²˜ëŸ¼ ì—„ì²­ í° ë°ì´í„°ì™€ ë°ì´í„° ì¦ì‹ ë•ë¶„ì— pre-trainingì´ í•„ìš”ê°€ ì—†ë‹¤.

ë‹¤ìŒìœ¼ë¡œëŠ” ì‘ì€ ëª¨ë¸ì´ ë” í° ì„±ëŠ¥ì„ ë°œíœ˜í–ˆë‹¤.

ë‹¤ìŒì€ multi-scale approachì¸ë° 40X with an additional input heatmapsê°€ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ë°œê²¬. ì´ ì¡°í•©ë“¤ì€ smoother heatmaps ì„ ë§Œë“œëŠ”ë° CNNì˜ ì´ë™ ë¶ˆë³€ì„±ê³¼ ì¸ì ‘í•œ ë§¤ì¹˜ë“¤ì˜ overlap ë•Œë¬¸ì´ë‹¤. ê·¸ë˜ì„œ ì´ëŸ¬í•œ ê°œì„ ì´ ì¢…ì–‘ì— ë‘˜ëŸ¬ìŒ“ì¸ ì‘ì€ ë¹„ ì¢…ì–‘ ì§€ì—­ë“¤ì„ ë“œëŸ¬ë‚¸ë‹¤. 